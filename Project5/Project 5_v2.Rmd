---
title: "Project 5 - Implementing a Recommender System on Spark"
author: "Dhairav Chhatbar"
date: "7/4/2020"
output: html_document
---

The goal of this project is give you practice beginning to work with a distributed recommender system. It is sufficient for this assignment to build out your application on a single node.  
  

Adapt one of your recommendation systems to work with Apache Spark and compare the performance with your previous iteration. Consider the efficiency of the system and the added complexity of using Spark. You may complete the assignment using PySpark (Python), SparkR (R), 
sparklyr (R), or Scala. 
  
Please include in your conclusion:  For your given recommender systemâ€™s data, algorithm(s), and (envisioned) implementation, at what point would you see moving to a distributed platform such as Spark becoming necessary?  
  
```{r, message=FALSE, warning=FALSE}
library(recommenderlab)
library(sparklyr)
library(dplyr)
library(tidyr)
library(kableExtra)
library(data.table)
library(ggplot2)
data("MovieLense")
```

### Datasetup
Create a training and test data sets
```{r}
eval_movies <- evaluationScheme(data=MovieLense, method="split", train=.80, 
                               given=min(rowCounts(MovieLense))-3, goodRating = 3)
```


### RecommenderLab Model
Build an Alternating Least Square (ALS) recommender system using Recommender Lab and capture system run times at the start and end of the build of the model and the prediction
```{r}
m1_s <- Sys.time()
rlab_m <- Recommender(getData(eval_movies, "train"), method = "ALS")
rlab_pred <- predict(rlab_m, getData(eval_movies, "known"), type = "ratings")
m1_f <- Sys.time()

m1_RMSE <- calcPredictionAccuracy(rlab_pred, getData(eval_movies, "unknown"))
```


### Spark Model
Build an Alternating Least Square (ALS) recommender system using Spark and capture system run times at the start and end of the build of the model and the prediction
```{r}
spark_c <- spark_connect(master = "local")

movie_id <- data.frame(item = MovieLenseMeta$title, title_ID = as.numeric(row.names(MovieLenseMeta)))

s_train <- getData.frame(getData(eval_movies, "train"))
s_train$user <- as.numeric(s_train$user)
s_train <- left_join(s_train, movie_id, by ="item")
spark_train <- copy_to(spark_c, s_train, overwrite = TRUE)

s_test <- getData.frame(getData(eval_movies, "known"))
s_test$user <- as.numeric(s_test$user)
s_test <- left_join(s_test, movie_id, by ="item")
spark_test <- copy_to(spark_c, s_test, overwrite = TRUE)

glimpse(spark_train)

m2_s <- Sys.time()
spark_m <- ml_als(spark_train, rating_col = "rating", user_col = "user", item_col = "title_ID", max_iter = 5)
spark_pred <- ml_transform(spark_m, spark_test) %>% collect()
m2_f <- Sys.time()
```

### Comparison
Comparing the RMSE and Runtimes of each, we see that not only Spark runs the algorithm faster but also procures a similar prediction. For massive datasets it is imperative to run a distributed system as it would not only be more time efficient but also financial efficient
```{r}
df <- data.frame(RMSE = c(m1_RMSE[1],RMSE(spark_pred$rating, spark_pred$prediction)), 
                 Runtime = c(m1_f-m1_s, m2_f-m2_s))
rownames(df) <- c("Recommender Lab", "Spark")
df
spark_disconnect(spark_c)
```

